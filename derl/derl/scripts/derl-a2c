#!/usr/bin/env python
"""
Script to run Advantage Actor Critic training.
"""
# pylint: disable=invalid-name
import tensorflow as tf
from tqdm import tqdm
import derl
tf.enable_eager_execution()


def a2c_atari_defaults():
  """ Returns default arguments for A2C training in Atari envs. """
  return {
      "nenvs": 8,
      "num-train-steps": 10e6,
      "num-runner-steps": 5,
      "gamma": 0.99,
      "lambda_": 1.,
      "normalize": dict(action="store_true"),
      "lr": 7e-4,
      "optimizer-decay": 0.99,
      "optimizer-epsilon": 1e-5,
      "value-loss-coef": 0.5,
      "entropy-coef": 0.01,
      "max-grad-norm": 0.5,
  }


def main():
  """ Runs A2C training. """
  args = derl.get_args(atari_defaults=a2c_atari_defaults())
  env = derl.env.nature_dqn_env(args.env_id, args.nenvs)

  policy = derl.ActorCriticPolicy(derl.NatureDQNModel([env.action_space.n, 1]))
  runner = derl.EnvRunner(env, policy, args.num_runner_steps,
                          transforms=[derl.GAE(policy, gamma=args.gamma,
                                               normalize=args.normalize,
                                               lambda_=args.lambda_),
                                      derl.MergeTimeBatch()])
  lr = derl.train.linear_anneal("lr", args.lr, args.num_train_steps,
                                step_var=runner.step_var)
  optimizer = tf.train.RMSPropOptimizer(lr, decay=args.optimizer_decay,
                                        epsilon=args.optimizer_epsilon)
  a2c = derl.A2C(policy, optimizer,
                 value_loss_coef=args.value_loss_coef,
                 entropy_coef=args.entropy_coef,
                 max_grad_norm=args.max_grad_norm)

  summary_writer = tf.contrib.summary.create_file_writer(args.logdir)
  summary_writer.set_as_default()
  pbar = tqdm(total=args.num_train_steps)
  with tf.contrib.summary.record_summaries_every_n_global_steps(
      args.log_period):
    while int(runner.step_var) < args.num_train_steps:
      pbar.update(int(runner.step_var) -  pbar.n)
      trajectory = runner.get_next()
      a2c.step(trajectory)

if __name__ == "__main__":
  main()
